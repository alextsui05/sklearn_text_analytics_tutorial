{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis Tutorial\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "I cloned the tutorial from the [scikit-learn github repo](https://github.com/scikit-learn/scikit-learn):\n",
    "\n",
    "`git clone git@github.com:scikit-learn/scikit-learn.git`\n",
    "\n",
    "Then I copied the tutorial files to my own working directory. I have scikit-learn installed through conda, so I should be good to go.\n",
    "\n",
    "## fetch_data.py\n",
    "\n",
    "The script pulls paragraph data from wikipedia pages in 11 different languages and stores them in the paragraphs folder. Also, for each paragraph pulled in, split it into 5-word chunks and save it as synthetic short paragraph data.\n",
    "\n",
    "### TIL\n",
    "\n",
    "* `np.array_split(array, n_groups)` - splits an array into n subarrays. I wonder what the equivalent for ruby's `each_slice(n)` is.\n",
    "* `urllib.request` module's `build_opener` constructs an object that can be used to make http requests.\n",
    "* `'%s_%04d.txt' % (lang, i)` - string interpolation format is good to review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started II\n",
    "\n",
    "Okay, seems like the above setup was unnecessary since we're using the 20 newsgroups dataset...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2257"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twenty_train.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.graphics'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names[twenty_train.target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Formulation\n",
    "\n",
    "* Input - Documents\n",
    "* Labels - Categories, such as `comp.graphics`\n",
    "* Features - Words in the document\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "* Assume you have a dictionary that assigns an integer to every word in your corpus.\n",
    "* Each document can be represented as a frequency vector\n",
    "  * The i-th entry corresponds to frequency in which the i-th word in the dictionary occurs\n",
    "* `scipy.sparse` efficiently represents this vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `CountVectorizer` builds a vocabulary from the input data and builds a frequency matrix\n",
    "- `(i, j)` gives the frequency of the `j`-th word in the vocabulary list in document `i`\n",
    "- `#vocabulary_` attribute is a dict mapping features (words) to indices\n",
    "- `#get_feature_names()` gives you the vocabulary list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf\n",
    "\n",
    "* Occurrences give too much weight to frequently occurring words\n",
    "* Term frequency (tf) - normalize a document's frequency vector by number of words in that document\n",
    "* Inverse-document frequency (idf) - downweight words frequently occurring in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "X_train_tfidf = tfidf_transformer.transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier on the labelled tfidf feature vectors\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# Use the classifier to classify new unseen text.\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predictions = classifier.predict(X_new_tfidf)\n",
    "for doc, category in zip(docs_new, predictions):\n",
    "    print('%r => %s' % (doc, twenty_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compose all the above into a reusable pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_classifier = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    "text_classifier.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8348868175765646"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate classifier accuracy on held-out test set\n",
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_classifier.predict(docs_test)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.60      0.74       319\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.97      0.81      0.88       396\n",
      "soc.religion.christian       0.65      0.99      0.78       398\n",
      "\n",
      "             micro avg       0.83      0.83      0.83      1502\n",
      "             macro avg       0.89      0.82      0.83      1502\n",
      "          weighted avg       0.88      0.83      0.84      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(twenty_test.target, predicted,\n",
    "    target_names=twenty_test.target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[192,   2,   6, 119],\n",
       "       [  2, 347,   4,  36],\n",
       "       [  2,  11, 322,  61],\n",
       "       [  2,   2,   1, 393]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(twenty_test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshooting\n",
    "\n",
    "```\n",
    "$ python fetch_data.py \n",
    "Traceback (most recent call last):\n",
    "  File \"fetch_data.py\", line 8, in <module>\n",
    "    import lxml.html\n",
    "  File \"/home/atsui/anaconda3/lib/python3.7/site-packages/lxml/html/__init__.py\", line 54, in <module>\n",
    "    from .. import etree\n",
    "ImportError: libicui18n.so.58: cannot open shared object file: No such file or directory\n",
    "```\n",
    "\n",
    "I guess conda is missing some necessary package.\n",
    "\n",
    "```\n",
    "$ conda search icu\n",
    "Loading channels: done\n",
    "# Name                       Version           Build  Channel             \n",
    "icu                             54.1               0  pkgs/free           \n",
    "icu                             58.2      h211956c_0  pkgs/main           \n",
    "icu                             58.2      h9c2bf20_1  pkgs/main       \n",
    "```\n",
    "\n",
    "I guess my problem will be solved by installing icu 58.2. Don't I already have it?\n",
    "\n",
    "```\n",
    "$ conda list | grep icu\n",
    "icu                       64.2                 he1b5a44_1    conda-forge\n",
    "```\n",
    "\n",
    "Will I get in trouble if I install an older version...?\n",
    "\n",
    "```\n",
    "$ conda install icu=58.2\n",
    "Collecting package metadata: done\n",
    "Solving environment: \\ \n",
    "The environment is inconsistent, please check the package plan carefully\n",
    "The following packages are causing the inconsistency:\n",
    "\n",
    "  - defaults/linux-64::conda-build==3.17.6=py37_0\n",
    "  - defaults/linux-64::lxml==4.2.5=py37hefd8a0e_0\n",
    "  - defaults/linux-64::libxml2==2.9.8=h26e45fe_1\n",
    "  - defaults/linux-64::qt==5.9.7=h5867ecd_1\n",
    "  - defaults/linux-64::qtconsole==4.4.3=py37_0\n",
    "  - conda-forge/linux-64::jupyter_contrib_nbextensions==0.5.1=py37_0\n",
    "  - defaults/linux-64::pango==1.42.4=h049681c_0\n",
    "  - defaults/linux-64::fontconfig==2.13.0=h9420a91_0\n",
    "  - defaults/linux-64::libarchive==3.3.3=h5d8350f_5\n",
    "  - defaults/linux-64::harfbuzz==1.8.8=hffaf4a1_0\n",
    "  - defaults/linux-64::libxslt==1.1.32=h1312cb7_0\n",
    "  - defaults/linux-64::anaconda==2018.12=py37_0\n",
    "  - defaults/linux-64::python-libarchive-c==2.8=py37_6\n",
    "  - defaults/linux-64::anaconda-navigator==1.9.6=py37_0\n",
    "  - defaults/linux-64::navigator-updater==0.2.1=py37_0\n",
    "  - defaults/linux-64::jupyter==1.0.0=py37_7\n",
    "  - defaults/linux-64::pyqt==5.9.2=py37h05f1152_2\n",
    "  - defaults/linux-64::spyder==3.3.2=py37_0\n",
    "  - defaults/linux-64::matplotlib==3.0.2=py37h5429711_0\n",
    "  - defaults/linux-64::cairo==1.14.12=h8948797_3\n",
    "  - defaults/linux-64::seaborn==0.9.0=py37_0\n",
    "  - defaults/linux-64::scikit-image==0.14.1=py37he6710b0_0\n",
    "done\n",
    "\n",
    "## Package Plan ##\n",
    "\n",
    "  environment location: /home/atsui/anaconda3\n",
    "\n",
    "  added / updated specs:\n",
    "    - icu=58.2\n",
    "\n",
    "\n",
    "The following packages will be downloaded:\n",
    "\n",
    "    package                    |            build\n",
    "    ---------------------------|-----------------\n",
    "    arrow-cpp-0.11.1           |   py37h5c3f529_1         6.7 MB\n",
    "    boost-cpp-1.67.0           |       h14c3975_4          11 KB\n",
    "    ca-certificates-2020.1.1   |                0         132 KB\n",
    "    certifi-2019.11.28         |           py37_0         156 KB\n",
    "    conda-4.8.3                |           py37_0         3.0 MB\n",
    "    conda-package-handling-1.6.0|   py37h7b6447c_0         872 KB\n",
    "    glog-0.3.5                 |       hf484d3e_1         158 KB\n",
    "    libboost-1.67.0            |       h46d08c1_4        20.9 MB\n",
    "    openssl-1.1.1e             |       h7b6447c_0         3.8 MB\n",
    "    pyarrow-0.11.1             |   py37he6710b0_0         1.9 MB\n",
    "    thrift-cpp-0.11.0          |       h02b749d_3         2.3 MB\n",
    "    ------------------------------------------------------------\n",
    "                                           Total:        40.0 MB\n",
    "\n",
    "The following NEW packages will be INSTALLED:\n",
    "\n",
    "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
    "  libboost           pkgs/main/linux-64::libboost-1.67.0-h46d08c1_4\n",
    "\n",
    "The following packages will be UPDATED:\n",
    "\n",
    "  ca-certificates    conda-forge::ca-certificates-2019.11.~ --> pkgs/main::ca-certificates-2020.1.1-0\n",
    "  conda                                       4.6.14-py37_0 --> 4.8.3-py37_0\n",
    "  openssl            conda-forge::openssl-1.1.1d-h516909a_0 --> pkgs/main::openssl-1.1.1e-h7b6447c_0\n",
    "\n",
    "The following packages will be SUPERSEDED by a higher-priority channel:\n",
    "\n",
    "  arrow-cpp          conda-forge::arrow-cpp-0.15.1-py37h98~ --> pkgs/main::arrow-cpp-0.11.1-py37h5c3f529_1\n",
    "  boost-cpp          conda-forge::boost-cpp-1.70.0-h8e57a9~ --> pkgs/main::boost-cpp-1.67.0-h14c3975_4\n",
    "  certifi                                       conda-forge --> pkgs/main\n",
    "  glog                   conda-forge::glog-0.4.0-he1b5a44_1 --> pkgs/main::glog-0.3.5-hf484d3e_1\n",
    "  icu                      conda-forge::icu-64.2-he1b5a44_1 --> pkgs/main::icu-58.2-h9c2bf20_1\n",
    "  pyarrow            conda-forge::pyarrow-0.15.1-py37h8b68~ --> pkgs/main::pyarrow-0.11.1-py37he6710b0_0\n",
    "  thrift-cpp         conda-forge::thrift-cpp-0.12.0-hf3afd~ --> pkgs/main::thrift-cpp-0.11.0-h02b749d_3\n",
    "  zstd                   conda-forge::zstd-1.4.4-h3b9ef0a_1 --> pkgs/main::zstd-1.3.7-h0b5b093_0\n",
    "```\n",
    "\n",
    "Let's just go for it.\n",
    "\n",
    "After installing icu 58.2, the python script worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
